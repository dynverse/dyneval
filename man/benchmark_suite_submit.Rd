% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/benchmark_suite.R
\name{benchmark_suite_submit}
\alias{benchmark_suite_submit}
\title{A benchmark suite with which to run all the methods on the different tasks}
\usage{
benchmark_suite_submit(tasks, task_group, task_fold, out_dir, remote_dir,
  timeout = 120, methods = get_descriptions(as_tibble = TRUE),
  designs = NULL, metrics = "correlation", extra_metrics = NULL,
  num_cores = 4, memory = "20G", max_wall_time = NULL,
  execute_before = NULL, r_module = "R", num_iterations = 20,
  num_init_params = 100, num_repeats = 1, do_it_local = FALSE,
  output_model = FALSE)
}
\arguments{
\item{tasks}{A tibble of tasks.}

\item{task_group}{A grouping vector for the different tasks.}

\item{task_fold}{A fold index vector for the different tasks.}

\item{out_dir}{A folder in which to output intermediate and final results.}

\item{remote_dir}{A folder in which to store intermediate results in a remote directory when using the PRISM package.}

\item{timeout}{The number of seconds 1 method has to solve each of the tasks before a timeout is generated.}

\item{methods}{A tibble of TI methods.}

\item{designs}{A names list of given designs data frames. Names must be equal to method short names.}

\item{metrics}{Which metrics to use;
see \code{\link{calculate_metrics}} for a list of which metrics are available.}

\item{extra_metrics}{Extra metrics to calculate but not evaluate with.}

\item{num_cores}{The number of cores to allocate per mlr run.}

\item{memory}{The memory to allocate per core.}

\item{max_wall_time}{The maximum amount of time each fold is allowed to run.}

\item{execute_before}{Shell commands to execute before running R}

\item{r_module}{Which R module to use in gridegine}

\item{num_iterations}{The number of iterations to run.}

\item{num_init_params}{The number of initial parameters to evaluate.}

\item{num_repeats}{The number of times to repeat the mlr process, for each group and each fold.}

\item{do_it_local}{Whether or not to run the benchmark suite locally (not recommended)}

\item{output_model}{Whether or not to return the outputted models}
}
\description{
A benchmark suite with which to run all the methods on the different tasks
}
